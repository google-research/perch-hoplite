{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVGgeTZLETYe"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-research/perch-hoplite/blob/main/perch_hoplite/agile/1_embed_audio_v2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-research/perch-hoplite/blob/main/perch_hoplite/agile/1_embed_audio_v2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QfDwaWNTZCZ"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook uses `perch-hoplite` to compute and save embeddings for a set of audio files using a pre-trained model. This is the first step in the agile modeling process. If the data you wish to search and classify is already embedded with a pre-trained model into a perch-hoplite database, then proceed to the step 2 colab notebook ([02_agile_modeling.ipynb](perch_hoplite/agile/02_agile_modeling.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Db84ySxSJYA"
      },
      "source": [
        "## [Optional] perch-hoplite installation for hosted runtimes\n",
        "\n",
        "If you have not already installed `perch-hoplite` (particularly if you are using a hosted Colab runtime), make sure to install `perch-hoplite` from the GitHub source to ensure the most recent version is installed. After installation, you will need to restart your runtime before running anything else. Go to the top menu, select `Runtime` then `Restart Session`.\n",
        "\n",
        "**If you want to use the Perch V2 model, you must additionally install TensorFlow version 2.20.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7bUZkS_Rawd"
      },
      "outputs": [],
      "source": [
        "# @title Only run this code if you need to install perch-hoplite\n",
        "\n",
        "!pip install git+https://github.com/google-research/perch-hoplite.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS5FgXBufrVt"
      },
      "outputs": [],
      "source": [
        "# @title If you plan to use Perch V2, you must install this version (or later) of TensorFlow and CUDA\n",
        "\n",
        "!pip install tensorflow[and-cuda]~=2.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTtVnkC-6_i7"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "from etils import epath\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "\n",
        "from perch_hoplite.agile import colab_utils\n",
        "from perch_hoplite.agile import embed\n",
        "from perch_hoplite.agile import source_info\n",
        "from perch_hoplite.db import brutalism\n",
        "from perch_hoplite.db import interface\n",
        "from perch_hoplite.zoo import taxonomy_model_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T4vILrO80iP"
      },
      "source": [
        "# Embed the audio data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6zdGxl68vft"
      },
      "outputs": [],
      "source": [
        "# @title Configuration {vertical-output: true}\n",
        "\n",
        "# @markdown Configure the raw dataset and output location(s).  The format is a mapping from\n",
        "# @markdown a dataset_name to a (base_path, fileglob) pair.  Note that the file\n",
        "# @markdown globs are case sensitive.  The dataset name can be anything you want.\n",
        "#\n",
        "# @markdown This structure allows you to move your data around without having to\n",
        "# @markdown re-embed the dataset.  The generated embedding database will be\n",
        "# @markdown placed in the base path. This allows you to simply swap out\n",
        "# @markdown the base path here if you ever move your dataset.\n",
        "\n",
        "# @markdown By default we only process one dataset at a time.  Re-run this entire notebook\n",
        "# @markdown once per dataset.\n",
        "\n",
        "# @markdown For example, we might set dataset_base_path to `/home/me/myproject`,\n",
        "# @markdown and use the glob `*/*.wav` if all of the audio files have filepaths\n",
        "# @markdown like `/home/me/myproject/site_XYZ/audio_ABC.wav` (e.g. audio files are contained\n",
        "# @markdown in subfolders of the base directory).\n",
        "\n",
        "# @markdown 1. Create a unique name for the database that will store the embeddings for the\n",
        "# @markdown target data.\n",
        "dataset_name = 'powdermill'  # @param {type: 'string'}\n",
        "\n",
        "# @markdown 2. Input the filepath for the folder that is containing the input audio files.\n",
        "dataset_base_path = 'gs://chirp-public-bucket/soundscapes/powdermill'  # @param {type: 'string'}\n",
        "\n",
        "# @markdown 3. Input the file pattern for the audio files within that folder that you want\n",
        "# @markdown to embed. Some examples for how to input:\n",
        "# @markdown - All files in the base directory of a specific type (not subdirectories): e.g.\n",
        "# @markdown `*.wav` (or `*.flac` etc) will generate embeddings for all .wav files (or whichever\n",
        "# @markdown format) in the `dataset_base_path`.\n",
        "# @markdown - All files in one level of subdirectories within the base directory: `*/*.flac`\n",
        "# @markdown will generate embeddings for all .flac files.\n",
        "# @markdown - Single file: `myfile.wav` will only embed the audio from that specific file.\n",
        "dataset_fileglob = '*/*.wav'  # @param {type:'string'}\n",
        "\n",
        "# @markdown 4. [Optional] If saving the embeddings database to a new directory, specify here.\n",
        "# @markdown Otherwise, leave blank - by default the embeddings database output will be saved\n",
        "# @markdown within `dataset_base_path` where the audio is located. You do not need to specify\n",
        "# @markdown `db_path` unless you want to maintain multiple distinct embedding databases, or\n",
        "# @markdown if you would like to save the output in a different folder. If your input audio\n",
        "# @markdown data is accessed from a public URL, we recommend specifying a separate output\n",
        "# @markdown directory here.\n",
        "db_path = '/tmp/hoplite'  # @param {type:'string'}\n",
        "if not db_path or db_path.lower() == 'none':\n",
        "  db_path = None\n",
        "\n",
        "# @markdown 5. Choose a supported model to generate embeddings. `perch_v2` is the latest Perch\n",
        "# @markdown model and was trained on multiple taxa include birds, mammals, insects and amphibians.\n",
        "# @markdown `perch_v2` has also demonstrated high performance for marine audio transfer learning\n",
        "# @markdown tasks. **NOTE: `perch_v2` only works with GPU runtimes - see above instructions.**\n",
        "# @markdown `perch_8` is the last updated version of Perch V1 trained only on birds, and\n",
        "# @markdown `birdnet_v2.3` is also a common option for birds. Other choices include `surfperch`\n",
        "# @markdown for coral reefs or `multispecies_whale` for marine mammals.\n",
        "model_choice = 'perch_v2'  # @param ['perch_v2', 'perch_8', 'humpback', 'multispecies_whale', 'surfperch', 'birdnet_V2.3']\n",
        "\n",
        "# @markdown 6. [Optional] Shard the audio for embeddings. File sharding automatically splits audio\n",
        "# @markdown files into smaller chunks for creating embeddings. This limits both system and GPU\n",
        "# @markdown memory usage, especially useful when working with long files (>1 hour).\n",
        "use_file_sharding = True  # @param {type:'boolean'}\n",
        "# @markdown If you want to change the length in seconds for the shards, specify here.\n",
        "shard_length_in_seconds = 60  # @param {type:'number'}\n",
        "\n",
        "audio_glob = source_info.AudioSourceConfig(\n",
        "    dataset_name=dataset_name,\n",
        "    base_path=dataset_base_path,\n",
        "    file_glob=dataset_fileglob,\n",
        "    min_audio_len_s=1.0,\n",
        "    target_sample_rate_hz=-2,\n",
        "    shard_len_s=float(shard_length_in_seconds) if use_file_sharding else None,\n",
        ")\n",
        "\n",
        "configs = colab_utils.load_configs(\n",
        "    source_info.AudioSources((audio_glob,)),\n",
        "    db_path,\n",
        "    model_config_key=model_choice,\n",
        "    db_key='sqlite_usearch',\n",
        ")\n",
        "configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN9Uyy1yqAWS"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the hoplite database (DB) {vertical-output: true}\n",
        "\n",
        "global db\n",
        "db = configs.db_config.load_db()\n",
        "num_embeddings = db.count_embeddings()\n",
        "\n",
        "print('Initialized DB located at:', configs.db_config.db_config.db_path)\n",
        "\n",
        "def drop_and_reload_db(_) -> interface.HopliteDBInterface:\n",
        "  db_path = epath.Path(configs.db_config.db_config.db_path)\n",
        "  for fp in db_path.glob('hoplite.sqlite*'):\n",
        "    fp.unlink()\n",
        "  (db_path / 'usearch.index').unlink()\n",
        "  print('\\n Deleted previous db at: ', configs.db_config.db_config.db_path)\n",
        "  db = configs.db_config.load_db()\n",
        "\n",
        "# @markdown If `drop_existing_db` set to True, when the database already exists and contains\n",
        "# @markdown embeddings, then those existing embeddings will be erased. You will be prompted\n",
        "# @markdown to confirm you wish to delete those existing embeddings. If you want to keep\n",
        "# @markdown existing embeddings in the database, then set to False, which will append the new\n",
        "# @markdown embeddings to the database.\n",
        "drop_existing_db = False  # @param {type: 'boolean'}\n",
        "\n",
        "if num_embeddings > 0 and drop_existing_db:\n",
        "  print('Existing DB contains projects: ', db.get_all_projects())\n",
        "  print('num embeddings: ', num_embeddings)\n",
        "  print('\\n\\nClick the button below to confirm you really want to drop the database at ')\n",
        "  print(f'{configs.db_config.db_config.db_path}\\n')\n",
        "  print(f'This will permanently delete all {num_embeddings} embeddings from the existing database.\\n')\n",
        "  print('If you do NOT want to delete this data, set `drop_existing_db` above to `False` and re-run this cell.\\n')\n",
        "\n",
        "  button = widgets.Button(description='Delete database?')\n",
        "  button.on_click(drop_and_reload_db)\n",
        "  display(button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnGWbhc0LhiU"
      },
      "outputs": [],
      "source": [
        "# @title Run the embedding {vertical-output: true}\n",
        "\n",
        "print(f'Embedding dataset as a new db project: {audio_glob.dataset_name}')\n",
        "\n",
        "worker = embed.EmbedWorker(\n",
        "    audio_sources=configs.audio_sources_config,\n",
        "    db=db,\n",
        "    model_config=configs.model_config,\n",
        ")\n",
        "\n",
        "worker.process_all(target_dataset_name=audio_glob.dataset_name)\n",
        "\n",
        "print('\\n\\nEmbedding complete, total embeddings: ', db.count_embeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvVuFw-somHe"
      },
      "outputs": [],
      "source": [
        "# @title Per project statistics {vertical-output: true}\n",
        "\n",
        "for project in db.get_all_projects():\n",
        "  window_ids = db.match_window_ids(\n",
        "      deployments_filter=config_dict.create(eq=dict(project=project))\n",
        "  )\n",
        "  print('Project:', project)\n",
        "  print('>>> num embeddings:', len(window_ids))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ihBNRbwuuwal"
      },
      "outputs": [],
      "source": [
        "# @title Show example embedding search\n",
        "# @markdown As an example (and to show that the embedding process worked), this selects a single\n",
        "# @markdown embedding from the database and outputs the embedding ids of the top-k (k = 128)\n",
        "# @markdown nearest neighbors in the database.\n",
        "\n",
        "q = db.get_embedding(db.match_window_ids(limit=1)[0])\n",
        "%time results, scores = brutalism.brute_search(worker.db, query_embedding=q, search_list_size=128, score_fn=np.dot)\n",
        "print([int(r.window_id) for r in results])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/chirp:colab_binary",
        "kind": "private"
      },
      "name": "01_embed_audio.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1ePT3-fDB3kA3_T7trthFtu8xTJQWQBoQ",
          "timestamp": 1723499538314
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
